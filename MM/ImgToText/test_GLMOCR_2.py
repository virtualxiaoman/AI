# import time
# import torch
# from PIL import Image
# from transformers import AutoProcessor, AutoModelForImageTextToText
#
# # 1. é…ç½®è·¯å¾„
# img_path = "G:/AAAé‡è¦æ–‡æ¡£ã€è¯ä¹¦ã€ç…§ç‰‡ã€å¯†é’¥/å›å¿†/æ¼”å”±ä¼š/æµå…‰åå¥-æ­¦æ±‰-25.12.27/ç¾¤å‹/æ— é”¡/æ— é”¡æ­Œå•.jpg"
# MODEL_PATH = "zai-org/GLM-OCR"
#
# # 2. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ (åŠ¡å¿…åŠ ä¸Š trust_remote_code)
# processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
# model = AutoModelForImageTextToText.from_pretrained(
#     MODEL_PATH,
#     torch_dtype=torch.bfloat16,
#     device_map="auto",
#     trust_remote_code=True
# ).eval()
#
# # 3. å‡†å¤‡è¾“å…¥æ•°æ®
# image = Image.open(img_path).convert("RGB")
# messages = [
#     {
#         "role": "user",
#         "content": [
#             {"type": "image"},
#             {"type": "text", "text": "Text Recognition:"}
#         ],
#     }
# ]
#
# # è¿™ä¸€æ­¥å°†å›¾ç‰‡å’Œæ–‡å­—è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„ tensor
# inputs = processor.apply_chat_template(
#     messages,
#     images=[image],  # å›¾ç‰‡å¯¹è±¡åœ¨è¿™é‡Œä¼ å…¥
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors="pt"
# ).to(model.device)
#
# # 4. æ¨ç†ä¸è®¡æ—¶
# if torch.cuda.is_available():
#     torch.cuda.synchronize()  # ç¡®ä¿ä¹‹å‰çš„æ‰€æœ‰æ˜¾å­˜æ“ä½œå·²å°±ç»ª
#
# start_time = time.time()
#
# with torch.no_grad():
#     generated_ids = model.generate(
#         **inputs,
#         max_new_tokens=8192,
#         do_sample=False  # OCR ä»»åŠ¡é€šå¸¸ä¸éœ€è¦éšæœºæ€§
#     )
#
# if torch.cuda.is_available():
#     torch.cuda.synchronize()  # ç­‰å¾… GPU è®¡ç®—å®Œæˆ
#
# end_time = time.time()
#
# # 5. è®¡ç®—ç»“æœ
# duration = end_time - start_time
# # è·å–ç”Ÿæˆçš„ Token æ•°é‡ï¼ˆæ’é™¤æ‰è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦ï¼‰
# generated_tokens_count = len(generated_ids[0]) - len(inputs["input_ids"][0])
# tokens_per_sec = generated_tokens_count / duration
#
# # 6. è§£ç ä¸è¾“å‡º
# output_text = processor.decode(generated_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
#
# print("-" * 30)
# print(f"è¯†åˆ«ç»“æœ:\n{output_text}")
# print("-" * 30)
# print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
# print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_per_sec:.2f} tokens/s")
# print(f"ğŸ”¢ ç”Ÿæˆ Token æ€»æ•°: {generated_tokens_count}")
